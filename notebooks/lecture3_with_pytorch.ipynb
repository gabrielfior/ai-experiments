{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f96267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import random\n",
    "%matplotlib inline\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85980d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=\"runs/experiment2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ba2e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e064f691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033 emma\n"
     ]
    }
   ],
   "source": [
    "words = utils.fetch_words()\n",
    "print(len(words), words[0])\n",
    "vocab_size = utils.vocabulary_size(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9ce27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize datasets\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "Xtr, Ytr = utils.build_dataset(words[:n1])\n",
    "Xdev, Ydev = utils.build_dataset(words[n1:n2])\n",
    "Xte, Yte = utils.build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4567ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_hidden):\n",
    "        super().__init__()\n",
    "        #self.embedding = nn.Embedding(vocab_size, n_embd),\n",
    "        self.net = nn.ModuleList([\n",
    "\n",
    "            #nn.Parameter(torch.randn((vocab_size, n_embd), generator=g)),\n",
    "\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=False),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.Tanh(),\n",
    "\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.Tanh(),\n",
    "\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.Tanh(),\n",
    "\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.Tanh(),\n",
    "\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.Tanh(),\n",
    "\n",
    "            nn.Linear(n_hidden, vocab_size, bias=False),\n",
    "            nn.BatchNorm1d(vocab_size),\n",
    "        ])\n",
    "        self.tanh_layers = [layer for layer in self.net if isinstance(layer, nn.Tanh)]\n",
    "        self.tanh_activations = [None] * len(self.tanh_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        activation_index = 0\n",
    "        for layer in self.net:\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.Tanh):\n",
    "                # Retain the graph for non-leaf Tensors to access their gradients\n",
    "                x.retain_grad()\n",
    "                self.tanh_activations[activation_index] = x\n",
    "                activation_index += 1\n",
    "        return x\n",
    "    \n",
    "    def log_tanh_stats(self, writer, step):\n",
    "        for i, act in enumerate(self.tanh_activations):\n",
    "            if act is not None and act.requires_grad:\n",
    "                writer.add_histogram(f\"Tanh/layer_{i+1}/activation\", act.detach().cpu(), step)\n",
    "                if act.grad is not None:\n",
    "                    writer.add_histogram(f\"Tanh/layer_{i+1}/gradient\", act.grad.detach().cpu(), step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a010bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we build the layers using Pytorch\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "block_size = 3\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# We replace torch.randn by nn.Parameter so that Pytorch tracks gradients automatically\n",
    "C = nn.Parameter(torch.randn((vocab_size, n_embd), generator=g))\n",
    "\n",
    "layers = MyMLP(vocab_size, n_embd, block_size, n_hidden)\n",
    "\n",
    "\n",
    "# Loss function and optimizer\n",
    "parameters = list(layers.parameters())\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "#optimizer = torch.optim.SGD(parameters, lr=0.001)\n",
    "# I am cheating below because Andrej doesn't use Adam, rather SGD.\n",
    "optimizer = torch.optim.AdamW(parameters, lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=0.5)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1c97654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yp/9rxjv3w91p3b3kh1v7prxz600000gn/T/ipykernel_32808/1206273740.py:48: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  if act.grad is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/10000, Loss: 3.5513,  LR: 0.001000\n",
      "Step 1001/10000, Loss: 2.3467,  LR: 0.001000\n",
      "Step 2001/10000, Loss: 2.6509,  LR: 0.001000\n",
      "Step 3001/10000, Loss: 2.3917,  LR: 0.001000\n",
      "Step 4001/10000, Loss: 2.0545,  LR: 0.001000\n",
      "Step 5001/10000, Loss: 2.3273,  LR: 0.001000\n",
      "Step 6001/10000, Loss: 1.8819,  LR: 0.001000\n",
      "Step 7001/10000, Loss: 2.4040,  LR: 0.001000\n",
      "Step 8001/10000, Loss: 2.5658,  LR: 0.001000\n",
      "Step 9001/10000, Loss: 2.2533,  LR: 0.001000\n",
      "Step 10000/10000, Loss: 2.1536,  LR: 0.000500\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 10000\n",
    "batch_size = 32\n",
    "\n",
    "for step in range(num_iterations):\n",
    "    # Sample a batch of data\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)  # example input: indices of tokens\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]  # input and target batches\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    x = emb.view(emb.shape[0], -1)\n",
    "    out = layers(x)\n",
    "\n",
    "    # Loss\n",
    "    loss = loss_fn(out, Yb)  # compute the loss\n",
    "    loss_value = loss.item()\n",
    "\n",
    "\n",
    "    #loss = F.cross_entropy(x, Yb) # loss function\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()  # zero the gradients\n",
    "    loss.backward()  # compute gradients\n",
    "    optimizer.step()  # update weights\n",
    "    scheduler.step()\n",
    "\n",
    "    # logging loss\n",
    "    writer.add_scalar(\"Loss/train\", loss_value, step)\n",
    "    writer.add_scalar(\"Learning_Rate\", optimizer.param_groups[0][\"lr\"], step)\n",
    "\n",
    "    # Log Tanh layer statistics\n",
    "    layers.log_tanh_stats(writer, step)\n",
    "\n",
    "    if step % 1000 == 0 or step == num_iterations - 1:\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f\"Step {step + 1}/{num_iterations}, Loss: {loss.item():.4f},  LR: {current_lr:.6f}\")\n",
    "\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d995cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.1556262969970703\n",
      "val 2.388932943344116\n",
      "test 2.4112343788146973\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  \n",
    "  layers.eval() \n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  logits = layers(embcat)\n",
    "  loss = loss_fn(logits, y)\n",
    "  print(split, loss.item())  \n",
    "  layers.train()  # optional: switch back to training mode\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "split_loss('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bd2c9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are tracking stuff on Tensorboard. I will add some screenshots to the Markdown file later for a blog post.\n",
    "# Note that we need a PyTorch model to track outputs of Tanh (among other things)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb2fc7e",
   "metadata": {},
   "source": [
    "![alt text](https://postimg.cc/H840z8b3 \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51115b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
